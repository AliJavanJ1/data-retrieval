{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین دوم</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ... آبان <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> \n",
    "مقدمه\n",
    "</h1>\n",
    "<p>\n",
    "در این تمرین قصد داریم به مباحث زیر بپردازیم:\n",
    "    <li>مدل‌های برداری</li>\n",
    "    <li>امتیازدهی و ارزیابی سیستم بازیابی</li>\n",
    "    <li>مدل‌های احتمالاتی</li>\n",
    "\n",
    "دیتاست مورد استفاده در این تمرین را می‌توانید در کنار این فایل مشاهده کنید. همچنین لطفا پس از اتمام تمرین یک بار از اول تا آخر نوت‌بوک را اجرا کنید تا مطمئن باشید تمام سل‌ها به درستی کار می‌کنند. \n",
    "\n",
    "کتابخانه‌های مورد نظرتان را هم می‌توانید در اولین سل نوت‌بوک فراخوانی کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, deque, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>1.\n",
    "آماده‌سازی دیتاست\n",
    "</h1>\n",
    "<p>\n",
    "با استفاده از تمرین قبل و عملیات‌هایی که در آنجا برای پیش‌پردازش متون پیاده‌سازی کردید، دیتاست داده شده را لود کنید. قرار است در ادامه با این دیتاست بخش‌های بعدی تمرین را پیاده‌سازی کنید. همچنین در صورتی که اجرای سل‌های بعدی برایتان طول کشید، می‌توانید آن را کوچک کنید. مثلا از ۷۰درصد دیتای آن استفاده کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paperId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40ea606185b59cd07b456cb1022d64bf41f5538d</th>\n",
       "      <td>Analysis on ground surface in ultrasonic face ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597c2d96c45e8ad83fc08e5d464d266b68f873ed</th>\n",
       "      <td>Measuring SARS-CoV-2 neutralizing antibody act...</td>\n",
       "      <td>The emergence of SARS-CoV-2 has created a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afbf330f0180320deff12fe42ded4f087b4a1811</th>\n",
       "      <td>A variational model for disocclusion</td>\n",
       "      <td>In this paper we study a variational approach ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa57e1cbaa46211499749cc75172e9ced26ad539</th>\n",
       "      <td>Helicoidally Arranged Polyacrylonitrile Fiber-...</td>\n",
       "      <td>In this study, we demonstrate the use of paral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780ce7c0d09d36d293def6be23e64b12c644169</th>\n",
       "      <td>The Interaction of SIRT4 and Calreticulin duri...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      title  \\\n",
       "paperId                                                                                       \n",
       "40ea606185b59cd07b456cb1022d64bf41f5538d  Analysis on ground surface in ultrasonic face ...   \n",
       "597c2d96c45e8ad83fc08e5d464d266b68f873ed  Measuring SARS-CoV-2 neutralizing antibody act...   \n",
       "afbf330f0180320deff12fe42ded4f087b4a1811               A variational model for disocclusion   \n",
       "fa57e1cbaa46211499749cc75172e9ced26ad539  Helicoidally Arranged Polyacrylonitrile Fiber-...   \n",
       "8780ce7c0d09d36d293def6be23e64b12c644169  The Interaction of SIRT4 and Calreticulin duri...   \n",
       "\n",
       "                                                                                   abstract  \n",
       "paperId                                                                                      \n",
       "40ea606185b59cd07b456cb1022d64bf41f5538d                                                     \n",
       "597c2d96c45e8ad83fc08e5d464d266b68f873ed  The emergence of SARS-CoV-2 has created a need...  \n",
       "afbf330f0180320deff12fe42ded4f087b4a1811  In this paper we study a variational approach ...  \n",
       "fa57e1cbaa46211499749cc75172e9ced26ad539  In this study, we demonstrate the use of paral...  \n",
       "8780ce7c0d09d36d293def6be23e64b12c644169                                                     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('data.csv', index_col='paperId')\n",
    "df = df.loc[:, ['title', 'abstract']]\n",
    "df.fillna('', inplace=True)\n",
    "df = df.reset_index().drop_duplicates(subset='paperId', keep='first').set_index('paperId')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfd1ff614284b2eb4e0b6c48b1bf81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'title': ['analysi',\n",
       "  'ground',\n",
       "  'surfac',\n",
       "  'ultrason',\n",
       "  'face',\n",
       "  'grind',\n",
       "  'silicon',\n",
       "  'carbid',\n",
       "  'sic',\n",
       "  'ceram',\n",
       "  'minor',\n",
       "  'vibrat',\n",
       "  'amplitud'],\n",
       " 'abstract': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        # Create a variable of stop words.\n",
    "        self.stopwords = list(stopwords.words(\"english\"))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # The main function of the class.\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.normalize(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        words = self.remove_digits_and_chars(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, words):\n",
    "        # Normalize text (lower case, stemming, lemmatization, etc.)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "        return words\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        # Remove links\n",
    "        return re.sub(r\"https?\\S+\", \"\", text)\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        # Remove punctutaions\n",
    "        return text.translate(\n",
    "            str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        )\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        # Tokenize text\n",
    "        words = word_tokenize(text)\n",
    "        return words\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "    def remove_digits_and_chars(sef, words):\n",
    "        words = [word for word in words if not word.isdigit() and word.isalpha()]\n",
    "        words = [word for word in words if len(word) > 1]\n",
    "        return words\n",
    "\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "corpus = defaultdict(lambda: {'title': [], 'abstract': []})\n",
    "for document_index, preprocessed_row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if document_index in corpus:\n",
    "        print(document_index)\n",
    "    corpus[document_index]['title'] = preprocessor.preprocess(preprocessed_row['title'])\n",
    "    corpus[document_index]['abstract'] = preprocessor.preprocess(preprocessed_row['abstract'])\n",
    "corpus[df.index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>1-1.\n",
    "نمایه‌سازی\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش باید برای سامانه یک\n",
    "Positional Index\n",
    "بسازید. \n",
    "<br>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا چکیده آن، در این قسمت باید نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید.\n",
    "با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره‌ی تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه‌ی جایگاه‌های این کلمه در هر بخش از هر سند را پیدا کرد.\n",
    "\n",
    "برای این بخش می‌توانید از نمایه‌ای که در تمرین اول زدید استفاده کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentBatch:\n",
    "    def __init__(self, corpus: dict[str, dict[str, list[str]]]):\n",
    "        self.documents = corpus\n",
    "        self.index = defaultdict(lambda: defaultdict(lambda: {'title': [], 'abstract': []}))\n",
    "\n",
    "    def index_construction(self):\n",
    "        # {TODO}: Build an inverted index for the batch\n",
    "        for document_index, preprocessed_row in self.documents.items():\n",
    "            title_tokens = preprocessed_row['title']\n",
    "            abstract_tokens = preprocessed_row['abstract']\n",
    "            for token_index, token in enumerate(title_tokens):\n",
    "                self.index[token][document_index]['title'].append(token_index)\n",
    "            for token_index, token in enumerate(abstract_tokens):\n",
    "                self.index[token][document_index]['abstract'].append(token_index)\n",
    "\n",
    "\n",
    "class FastSearchEngine:\n",
    "    def __init__(self):\n",
    "        self.main_index = defaultdict(lambda: defaultdict(lambda: {'title': [], 'abstract': []}))\n",
    "        self.daily_indices = deque()\n",
    "\n",
    "    def add_batch(self, batch: DocumentBatch):\n",
    "        # {TODO}: Incorporate the new batch into the daily indices\n",
    "        batch.index_construction()\n",
    "        self.daily_indices.append(batch.index)\n",
    "        \n",
    "    def end_of_day_logarithmic_merge(self):\n",
    "        # {TODO}: Implement the logarithmic merge strategy\n",
    "        while self.daily_indices:\n",
    "            index = self.daily_indices.pop()\n",
    "            for key, value in index.items():\n",
    "                self.main_index[key].update(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f557f9c6814d2a96913fccbd99ad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': [], 'abstract': [46, 154, 155]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_positional_indexes(corpus: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Constructs positional indexes for the processed data by inserting words into a trie and creating positional indexes and posting lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : dict\n",
    "        The processed data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The constructed positional indexes.\n",
    "    \"\"\"\n",
    "\n",
    "    postional_index = defaultdict(lambda: defaultdict(lambda: {'title': [], 'abstract': []}))\n",
    "    # TODO: Insert words from the processed data into a trie and construct positional indexes and posting lists.\n",
    "    search_engine = FastSearchEngine()\n",
    "    servers_number = 5\n",
    "    days_number = 5\n",
    "    servers_item_list = []\n",
    "    items_list = list(corpus.items())\n",
    "    for i in range(servers_number):\n",
    "        servers_item_list.append(items_list[i::servers_number])\n",
    "    for i in tqdm(range(days_number)):\n",
    "        for j in range(servers_number):\n",
    "            batch = DocumentBatch(defaultdict(lambda: {'title': [], 'abstract': []}, servers_item_list[j][i::days_number]))\n",
    "            search_engine.add_batch(batch)\n",
    "        search_engine.end_of_day_logarithmic_merge()\n",
    "\n",
    "    postional_index.update(search_engine.main_index)\n",
    "    return postional_index\n",
    "\n",
    "docs = construct_positional_indexes(corpus)\n",
    "print(corpus[df.index[0]]['title'][0])\n",
    "docs[corpus[df.index[0]]['title'][0]]['871057177837ffacc315d31bb9f2dcaa705a0ff0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>2.\n",
    "مدل‌های برداری و احتمالاتی\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش قصد داریم تا با استفاده از مدل‌های برداری و احتمالاتی، دو سیستم بازیابی اطلاعات طراحی کنیم. در نهایت قرار است سیستم‌های طراحی شده در این بخش را مورد ارزیابی قرار دهیم.\n",
    "\n",
    "در مدل‌های برداری ما به ازای هر داک که در اختیار داریم و کوئری ورودی یک بردار در فضا در نظر می‌گیریم. در ادامه برای بدست آوردن میزان ارتباط داک‌ها و کوئری، از معیارهایی مانند ضرب داخلی بردارها استفاده می‌کنیم. در این بخش ابتدا بردارهای مربوط به هرکدام را با استفاده از مقادیر \n",
    "tf, idf\n",
    "می‌سازیم و سپس به سراغ محاسبه امتیاز داک‌ها می‌رویم تا در نهایت بتوانیم با استفاده از این امتیازها داک‌هایی مرتبط‌تر را خروجی دهیم و یک سیستم بازیابی کامل را طراحی کنیم. \n",
    "\n",
    "در مدل‌های احتمالاتی اساس کار محاسبه \n",
    "$P(R | d, q)$\n",
    "است. در این روش می‌خواهیم به نوعی داک‌ها را به صورت مدل‌های احتمالاتی ببینیم. در درس مشاهده کردید که در انتها مدل‌های احتمالاتی هم رفتاری مشابه با مدل‌های برداری دارند. در ادامه یک سیستم مبتنی بر این مدل‌ها و با روش \n",
    "Okapi25\n",
    "طراحی می‌کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>2-1.\n",
    "ساخت ماتریس TF-IDF\n",
    "</h2>\n",
    "<p>\n",
    "در این مرحله دو تابع\n",
    "<code dir=\"ltr\">get_tf(token, doc_id)</code>\n",
    "و\n",
    "<code dir=\"ltr\">get_idf(token)</code>\n",
    "را پیاده‌سازی کنید که تابع اول مقدار\n",
    "tf\n",
    "توکن ورودی را در شناسه‌ی موردنظر\n",
    "و تابع دوم مقدار\n",
    "idf\n",
    "توکن ورودی را برروی نمایه‌ی کنونی حساب می‌کند و آن را به عنوان خروجی برمی‌گرداند.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "$$tf_{t, d} = \\text{Numbers of } t \\text{ in the title of document } d + \\text{Numbers of } t \\text{ in the abstract of document } d$$\n",
    "$$idf_t = \\log\\Biggl(\\frac{\\text{Number of documents}}{\\text{Number of documents that contains } t + 1}\\Biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tf(token: str, positional_index: dict, doc_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the term frequency (TF) of a given token within a specific document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The token for which to retrieve the term frequency.\n",
    "    positional_index : dict\n",
    "        A positional index that maps tokens to their positions in documents.\n",
    "    doc_id : str\n",
    "        The unique identifier of the document in which to calculate the TF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The term frequency (TF) of the given token within the specified document.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement the calculation of term frequency (TF) for the token in the specified document.\n",
    "    return len(positional_index[token][doc_id]['title']) + len(positional_index[token][doc_id]['abstract'])\n",
    "\n",
    "get_tf('deep', docs, '40ea606185b59cd07b456cb1022d64bf41f5538d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4873356041034036"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_idf(token: str, positional_index: dict, corpus: dict) -> float:\n",
    "    \"\"\"\n",
    "    Retrieves the inverse document frequency (IDF) of a given token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The token to retrieve the IDF for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The IDF of the given token.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement the calculation of IDF for the token based on your document collection.\n",
    "    return np.log(len(corpus) / (len(positional_index[token]) + 1))\n",
    "\n",
    "get_idf('deep', docs, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "در مرحله‌ی نهایی تابع\n",
    "<code dir=\"ltr\">generate_tfidf_list(corpus, positional_index)</code>\n",
    "را پیاده‌سازی کنید که با ورودی گرفتن\n",
    "corpus\n",
    "و\n",
    "positional_index\n",
    "مقادیر\n",
    "tf-idf\n",
    "را برای هر\n",
    "token\n",
    "به دست می‌آورد و به شکل یک ماتریس به عنوان خروجی برمی‌گرداند.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ca02448f6042adaa4e3780c00e3c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.generate_tfidf_list.<locals>.<lambda>()>,\n",
       "            {'analysi': 1.804946473510009,\n",
       "             'ground': 4.797733236834243,\n",
       "             'surfac': 3.5820643927451155,\n",
       "             'ultrason': 6.783648720503256,\n",
       "             'face': 4.310718261761971,\n",
       "             'grind': 7.120120957124469,\n",
       "             'silicon': 5.896345525502353,\n",
       "             'carbid': 8.036411688998623,\n",
       "             'sic': 7.630946580890459,\n",
       "             'ceram': 5.9569701473187875,\n",
       "             'minor': 5.2031983449424075,\n",
       "             'vibrat': 5.9569701473187875,\n",
       "             'amplitud': 5.510683044690368})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_tfidf_list(corpus: dict, positional_index: dict) -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generates a list of dictionaries representing documents with associated TF-IDF scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : dict\n",
    "        The corpus containing the processed data.\n",
    "    positional_index : dict\n",
    "        The positional index containing the term frequencies and document frequencies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, float]]\n",
    "        The list of dictionaries representing documents with associated TF-IDF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Write the function to generate the list of dictionaries with TF-IDF scores.\n",
    "    tfidf_list = []\n",
    "    for doc_id, doc in tqdm(corpus.items()):\n",
    "        tfidf = defaultdict(lambda: 0)\n",
    "        for token in doc['title'] + doc['abstract']:\n",
    "            tfidf[token] = get_tf(token, positional_index, doc_id) * get_idf(token, positional_index, corpus)\n",
    "        tfidf_list.append(tfidf)\n",
    "    return tfidf_list\n",
    "\n",
    "tfidf = generate_tfidf_list(corpus, docs)\n",
    "tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbid          \t8.036411688998623\t(normalized: 0.3829479102083639)\n",
      "sic             \t7.630946580890459\t(normalized: 0.3636268472985346)\n",
      "grind           \t7.120120957124469\t(normalized: 0.33928518678232916)\n",
      "ultrason        \t6.783648720503256\t(normalized: 0.32325174488765646)\n",
      "ceram           \t5.9569701473187875\t(normalized: 0.283859184592569)\n",
      "vibrat          \t5.9569701473187875\t(normalized: 0.283859184592569)\n",
      "silicon         \t5.896345525502353\t(normalized: 0.28097032409982475)\n",
      "amplitud        \t5.510683044690368\t(normalized: 0.2625928881510294)\n",
      "minor           \t5.2031983449424075\t(normalized: 0.24794074889455978)\n",
      "ground          \t4.797733236834243\t(normalized: 0.22861968598473048)\n",
      "face            \t4.310718261761971\t(normalized: 0.20541264107942597)\n",
      "surfac          \t3.5820643927451155\t(normalized: 0.17069111520398728)\n",
      "analysi         \t1.804946473510009\t(normalized: 0.08600859523098188)\n"
     ]
    }
   ],
   "source": [
    "def show_document_vector(vector: list[dict[str, float]], doc_index: int) -> None:\n",
    "    \"\"\"\n",
    "    Prints the non-zero weights and corresponding terms for a document represented as a vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : list of dict of str:float\n",
    "        The list of document vectors, where each document vector is a dictionary with term weights.\n",
    "    doc_id : int\n",
    "        The document index for which to display the vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    vector_list = [(term, weight) for term, weight in vector[doc_index].items() if weight > 0]\n",
    "    vector_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    length = math.sqrt(sum(weight ** 2 for _, weight in vector_list))\n",
    "    normalized = {term: tfidf / length for term, tfidf in vector_list}\n",
    "    for term, tfidf in vector_list:\n",
    "        print(f'{term.ljust(16)}\\t{tfidf}\\t(normalized: {normalized[term]})')\n",
    "\n",
    "show_document_vector(tfidf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>2-2.\n",
    "امتیازدهی به سندها\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش می‌خواهیم تا با سه شیوه‌ی متفاوت، داکیومنت‌ها را امتیازدهی کرده و kتا با بالاترین امتیاز را برگردانیم.\n",
    "روش‌ها به شکل زیر است:\n",
    "\n",
    "- **ltc.lnc:** در این روش از cosine similarity استفاده می‌کنیم و داده‌ها را بر اساس آن مرتّب می‌کنیم. برای متوجّه شدن نحوه‌ی عملکرد این سیستم، می‌توانید به اسلایدهای درس مراجعه کنید. همچنین دقت کنید که لازم است این امتیازدهی را هم به روش \n",
    "term-at-a-time\n",
    "و هم به روش \n",
    "doc-at-a-time\n",
    "محاسبه کنید.\n",
    "\n",
    "- **Okapi25:** برای پیاده‌سازی این روش از تساوی زیر استفاده کنید:\n",
    "</p>\n",
    "</div>\n",
    "<div>\n",
    "<p>\n",
    "\n",
    "$$ RSV_d = \\sum_{t \\in q} idf(t)\\times\\frac{(k_1 + 1)tf(t, d)}{k_1((1 - b) + b\\frac{dl(d)}{avg(dl)}) + tf(t, d)} $$\n",
    "</p>\n",
    "</div>\n",
    "<div dir='rtl'>\n",
    "<p>\n",
    "برای محاسبه این دو معیار، از توابعی که در بخش قبل پیاده‌سازی کردید استفاده کنید.\n",
    "\n",
    "در تابع search که مربوط به جست و جوی پرسمان کلی است، به عنوان ورودی پارامتر پرسمان (query)، روش محاسبه امتیاز (method)، تعداد اسنادی که باید برگردانده شود (max-result-count) را ورودی می گیرید. ورودی وزن \n",
    "(weight)\n",
    "نشان می‌دهد که امتیاز نهایی چه وزنی از امتیاز عنوان و چکیده دارد. به زبان دیگر:\n",
    "</p>\n",
    "</div>\n",
    "<div>\n",
    "<p>\n",
    "\n",
    "$$ final\\_score = weight \\times title\\_score + (1 - weight) \\times abstract\\_score $$\n",
    "</p>\n",
    "</div>\n",
    "<div dir='rtl'>\n",
    "<p>\n",
    "\n",
    "**توجّه کنید** که تابع نوشته شده، صرفاً یک prototype است و تا وقتی که نیازمندی‌های گفته شده را پیاده کنید، تغییر تابع مشکلی ندارد.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "# پاسخ:\n",
    "### با توجه به اینکه تابع های قبلی با نیاز های مسئله مطابقت ندارد در ادامه برای امتیاز دهی به سند‌‌ها، اینجانب به ازای هر روش، یک dataframe جداگانه محاسبه میکنم و در هنگام مشخص شدن مقدار پرسش به کمک آنها امتیاز سند را مشخص می‌نمایم \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c0fdb931904050a9a278ccb4123afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">40ea606185b59cd07b456cb1022d64bf41f5538d</th>\n",
       "      <th colspan=\"2\" halign=\"left\">597c2d96c45e8ad83fc08e5d464d266b68f873ed</th>\n",
       "      <th colspan=\"2\" halign=\"left\">afbf330f0180320deff12fe42ded4f087b4a1811</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fa57e1cbaa46211499749cc75172e9ced26ad539</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8780ce7c0d09d36d293def6be23e64b12c644169</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">41156a34679d10200ad4bc5ba3fb391c484417ab</th>\n",
       "      <th colspan=\"2\" halign=\"left\">7cc4ecd09745b8aec00bfe07618db4d6be05cca2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">188a93c03c4db17c487ccc9419864d341acf0bb9</th>\n",
       "      <th colspan=\"2\" halign=\"left\">812bf641028468bcfe95d31d515129728e067d99</th>\n",
       "      <th colspan=\"2\" halign=\"left\">idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>interact</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.085168</td>\n",
       "      <td>2.778916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 12368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         40ea606185b59cd07b456cb1022d64bf41f5538d           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         597c2d96c45e8ad83fc08e5d464d266b68f873ed           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         afbf330f0180320deff12fe42ded4f087b4a1811           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         fa57e1cbaa46211499749cc75172e9ced26ad539           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         8780ce7c0d09d36d293def6be23e64b12c644169           ...  \\\n",
       "                                            title abstract  ...   \n",
       "interact                                      1.0      0.0  ...   \n",
       "\n",
       "         41156a34679d10200ad4bc5ba3fb391c484417ab           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         7cc4ecd09745b8aec00bfe07618db4d6be05cca2           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         188a93c03c4db17c487ccc9419864d341acf0bb9           \\\n",
       "                                            title abstract   \n",
       "interact                                      0.0      0.0   \n",
       "\n",
       "         812bf641028468bcfe95d31d515129728e067d99                idf            \n",
       "                                            title abstract     title  abstract  \n",
       "interact                                      0.0      0.0  4.085168  2.778916  \n",
       "\n",
       "[1 rows x 12368 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens_tf_idf_matrix():\n",
    "    cols = pd.MultiIndex.from_product([list(corpus.keys()) + ['idf'], ['title', 'abstract']])\n",
    "    data = np.zeros((len(docs.keys()), len(cols)))\n",
    "    tokens_tf_idf_matrix = pd.DataFrame(data, index=docs.keys(), columns=cols)\n",
    "    tokens_tf_idf_matrix.columns = cols\n",
    "\n",
    "    indexes_place = {index: ind_of_index for ind_of_index, index in enumerate(tokens_tf_idf_matrix.index)}\n",
    "    columns_place = {}\n",
    "    for index, (doc_id, doc_part) in enumerate(tokens_tf_idf_matrix.columns):\n",
    "        if doc_id not in columns_place:\n",
    "            columns_place[doc_id] = {}\n",
    "        columns_place[doc_id][doc_part] = index\n",
    "        \n",
    "    tf_idf_matrix_np = tokens_tf_idf_matrix.to_numpy()\n",
    "\n",
    "    for token, doc_dict in tqdm(docs.items()):\n",
    "        token_df = {'title': 0, 'abstract': 0}\n",
    "        for doc_id, incidence_dict in doc_dict.items():\n",
    "            for document_part, incidence_list in incidence_dict.items():\n",
    "                index = indexes_place[token]\n",
    "                column = columns_place[doc_id][document_part]\n",
    "                tf_idf_matrix_np[index, column] = len(incidence_list)\n",
    "                if incidence_list:\n",
    "                    token_df[document_part] += 1\n",
    "        for document_part, document_part_token_df in token_df.items():\n",
    "            index = indexes_place[token]\n",
    "            column = columns_place['idf'][document_part]\n",
    "            tf_idf_matrix_np[index, column] = np.log(len(corpus) / (document_part_token_df + 1))\n",
    "            \n",
    "    return tokens_tf_idf_matrix\n",
    "\n",
    "tokens_tf_idf_matrix = get_tokens_tf_idf_matrix()\n",
    "tokens_tf_idf_matrix.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_ind(*args):\n",
    "    return pd.MultiIndex.from_product(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18783/3293177143.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  docs_tf_idf_matrix[:] = 1 + np.where(docs_tf_idf_matrix.to_numpy() > 0, np.log(docs_tf_idf_matrix.to_numpy()), 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs_tf_idf_matrix():\n",
    "    docs_tf_idf_matrix = tokens_tf_idf_matrix[corpus.keys()].copy(deep=True)\n",
    "\n",
    "    # TF calculation\n",
    "    docs_tf_idf_matrix[:] = 1 + np.where(docs_tf_idf_matrix.to_numpy() > 0, np.log(docs_tf_idf_matrix.to_numpy()), 0)\n",
    "\n",
    "    # IDF multiplication\n",
    "    np_docs_title_tf_idf_matrix = docs_tf_idf_matrix[df_ind(corpus.keys(), ['title'])].to_numpy()\n",
    "    np_docs_abstract_tf_idf_matrix = docs_tf_idf_matrix[df_ind(corpus.keys(), ['abstract'])].to_numpy()\n",
    "\n",
    "    np_docs_title_tf_idf_matrix = np_docs_title_tf_idf_matrix * tokens_tf_idf_matrix[('idf', 'title')].to_numpy()[:, np.newaxis]\n",
    "    np_docs_abstract_tf_idf_matrix = np_docs_abstract_tf_idf_matrix * tokens_tf_idf_matrix[('idf', 'abstract')].to_numpy()[:, np.newaxis]\n",
    "\n",
    "    docs_tf_idf_matrix[df_ind(corpus.keys(), ['title'])] = np_docs_title_tf_idf_matrix\n",
    "    docs_tf_idf_matrix[df_ind(corpus.keys(), ['abstract'])] = np_docs_abstract_tf_idf_matrix\n",
    "\n",
    "    # norm caculation\n",
    "    np_docs_tf_idf_matrix = docs_tf_idf_matrix.to_numpy()\n",
    "\n",
    "    docs_tf_idf_matrix[:] = np.divide(\n",
    "        np_docs_tf_idf_matrix,\n",
    "        (np_docs_tf_idf_matrix**2).sum(axis=0)[np.newaxis, :]**(1/2)\n",
    "    )\n",
    "    \n",
    "    return docs_tf_idf_matrix\n",
    "\n",
    "docs_tf_idf_matrix = get_docs_tf_idf_matrix()\n",
    "# show final result\n",
    "(docs_tf_idf_matrix.to_numpy()**2).sum(axis=0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.75\n",
    "k1 = np.mean([1.2, 2])\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_avg_doc_len_and_len_corpus():\n",
    "    len_corpus = {}\n",
    "    mean_doc_len = {'title': 0, 'abstract': 0}\n",
    "    for doc_id, doc_corpus in corpus.items():\n",
    "        len_corpus[doc_id] = {}\n",
    "        for doc_part_label, doc_part_list in doc_corpus.items():\n",
    "            len_corpus[doc_id][doc_part_label] = len(doc_part_list)\n",
    "            mean_doc_len[doc_part_label] += len(doc_part_list)\n",
    "    mean_doc_len['title'] /= len(corpus)\n",
    "    mean_doc_len['abstract'] /= len(corpus)\n",
    "    \n",
    "    return mean_doc_len, len_corpus\n",
    "\n",
    "def get_np_docs_rsv_matrix():\n",
    "    mean_doc_len, len_corpus = get_avg_doc_len_and_len_corpus()\n",
    "\n",
    "    dividend_coefficient = k1 + 1\n",
    "    doc_divisor_reminder = {'title': [], 'abstract': []}\n",
    "    for doc_id in corpus.keys():\n",
    "        doc_len_corpus = len_corpus[doc_id]\n",
    "        for doc_part_label, doc_part_len in doc_len_corpus.items():\n",
    "            doc_divisor_reminder[doc_part_label].append(k1 * (1-b + b * doc_part_len / mean_doc_len[doc_part_label]))\n",
    "\n",
    "    docs_rsv_matrix = tokens_tf_idf_matrix[corpus.keys()].copy(deep=True)\n",
    "\n",
    "    np_docs_rsv_matrix = docs_rsv_matrix.to_numpy()\n",
    "    np_docs_rsv_matrix[:, 0::2] = np.divide(\n",
    "        np_docs_rsv_matrix[:, 0::2] * dividend_coefficient,\n",
    "        (\n",
    "            np_docs_rsv_matrix[:, 0::2] \n",
    "            + np.array(doc_divisor_reminder['title'])[np.newaxis, :]\n",
    "        )\n",
    "    )* tokens_tf_idf_matrix['idf']['title'].to_numpy()[:, np.newaxis]\n",
    "    np_docs_rsv_matrix[:, 1::2] = np.divide(\n",
    "        np_docs_rsv_matrix[:, 1::2] * dividend_coefficient, \n",
    "        (\n",
    "            np_docs_rsv_matrix[:, 1::2] \n",
    "            + np.array(doc_divisor_reminder['abstract'])[np.newaxis, :]\n",
    "        )\n",
    "    ) * tokens_tf_idf_matrix['idf']['abstract'].to_numpy()[:, np.newaxis]\n",
    "    \n",
    "    return docs_rsv_matrix\n",
    "\n",
    "docs_rsv_matrix = get_np_docs_rsv_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_lnc(query: str):\n",
    "    q_tokens = preprocessor.preprocess(query)\n",
    "    q_tokens_dict = defaultdict(lambda: 0)\n",
    "    q_tokens_dict.update(Counter(q_tokens))\n",
    "    q_vector_square_sum = 0\n",
    "    for token, count in q_tokens_dict.items():\n",
    "        q_tokens_dict[token] = (1 + np.log(count))\n",
    "        q_vector_square_sum += q_tokens_dict[token]**2\n",
    "    q_vector_length = q_vector_square_sum**(1/2)\n",
    "    for token, tf in q_tokens_dict.items():\n",
    "        q_tokens_dict[token] = tf / q_vector_length\n",
    "    \n",
    "    return q_tokens_dict\n",
    "\n",
    "def search(query: str, max_result_count: int, method: str = 'ltc-lnc-tat', weight: float = 0.5):\n",
    "    \"\"\"\n",
    "    Finds relevant documents to query\n",
    "\n",
    "    Parameters\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
    "                        notice that if max_result_count = -1, then you have to return all docs\n",
    "\n",
    "    method: 'ltc-lnc-tat' or 'ltc-lnc-dat' or 'okapi25'\n",
    "\n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    list\n",
    "    Retreived documents with snippet\n",
    "    \"\"\"\n",
    "    scores = defaultdict(lambda: 0)\n",
    "    if method == 'ltc-lnc-tat':\n",
    "        q_tokens_dict = get_query_lnc(query)\n",
    "        for token, lnc in q_tokens_dict.items():\n",
    "            for doc_id in corpus.keys():\n",
    "                scores[doc_id] += (\n",
    "                    weight * lnc * docs_tf_idf_matrix[doc_id, 'title'][token] \n",
    "                    + (1 - weight) * lnc * docs_tf_idf_matrix[doc_id, 'abstract'][token]\n",
    "                )\n",
    "    elif method == 'ltc-lnc-dat':\n",
    "        q_tokens_dict = get_query_lnc(query)\n",
    "        for doc_id in corpus.keys():\n",
    "            for token, lnc in q_tokens_dict.items():\n",
    "                scores[doc_id] += (\n",
    "                    weight * lnc * docs_tf_idf_matrix[doc_id, 'title'][token] \n",
    "                    + (1 - weight) * lnc * docs_tf_idf_matrix[doc_id, 'abstract'][token]\n",
    "                )\n",
    "    else:\n",
    "        q_tokens = preprocessor.preprocess(query)\n",
    "        for doc_id in corpus.keys():\n",
    "            for token in q_tokens:\n",
    "                scores[doc_id] += (\n",
    "                    weight * docs_rsv_matrix[doc_id, 'title'][token] \n",
    "                    + (1 - weight) * docs_rsv_matrix[doc_id, 'abstract'][token]\n",
    "                )\n",
    "    top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:max_result_count]\n",
    "    return [doc_id for doc_id, _ in top_k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>3.\n",
    "مدل‌های زبانی\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش یک سیستم مبتنی بر مدل‌های زبانی پیاده‌سازی می‌کنید. مدل زبانی‌ای که باید استفاده کنید مدل \n",
    "unigram\n",
    "است که می‌توان گفت ساده‌ترین مدل ممکن است. در این مدل ما برای هر داک یک مدل دظر نظر می‌گیریم و احتمال حضور کلمات در مدل یک داک \n",
    "نسبت به هم مستقل هستند. در پیاده‌سازی این بخش دستتان خیلی باز است. مدل‌هایی که تشکیل می‌دهید و نحوه \n",
    "smoothing \n",
    "و همچنین پارامترهای مدل همگی به خودتان برمی‌گردد. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_dirichlet_smoothing_matrix():\n",
    "    alpha = 0.5\n",
    "\n",
    "    tokens_dirichlet_smoothing_matrix = tokens_tf_idf_matrix[corpus.keys()].copy(deep=True)\n",
    "    \n",
    "    np_tokens_dirichlet_smoothing_matrix = tokens_dirichlet_smoothing_matrix.to_numpy()\n",
    "    np_title_collection_freq_matrix = np_tokens_dirichlet_smoothing_matrix[:, 0::2].sum(axis=1)\n",
    "    np_abstract_collection_freq_matrix = np_tokens_dirichlet_smoothing_matrix[:, 1::2].sum(axis=1)\n",
    "    np_title_dirichlet_smoothing_matrix = np_title_collection_freq_matrix / np_title_collection_freq_matrix.sum()\n",
    "    np_abstract_dirichlet_smoothing_matrix = np_abstract_collection_freq_matrix / np_abstract_collection_freq_matrix.sum()\n",
    "\n",
    "\n",
    "    _, len_corpus = get_avg_doc_len_and_len_corpus()\n",
    "\n",
    "    len_corpus_np = {'title': [], 'abstract': []}\n",
    "    for doc_id in corpus.keys():\n",
    "        doc_len_corpus = len_corpus[doc_id]\n",
    "        for doc_part_label, doc_part_len in doc_len_corpus.items():\n",
    "            len_corpus_np[doc_part_label].append(doc_part_len)\n",
    "    len_corpus_np['title'] = np.array(len_corpus_np['title'])\n",
    "    len_corpus_np['abstract'] = np.array(len_corpus_np['abstract'])\n",
    "\n",
    "\n",
    "    np_tokens_dirichlet_smoothing_matrix[:, 0::2] += alpha * np_title_dirichlet_smoothing_matrix[:, np.newaxis] \n",
    "    np_tokens_dirichlet_smoothing_matrix[:, 1::2] += alpha * np_abstract_dirichlet_smoothing_matrix[:, np.newaxis] \n",
    "\n",
    "    np_tokens_dirichlet_smoothing_matrix[:, 0::2] = np.divide(\n",
    "        np_tokens_dirichlet_smoothing_matrix[:, 0::2],\n",
    "        len_corpus_np['title'][np.newaxis, :] + alpha\n",
    "    )\n",
    "    np_tokens_dirichlet_smoothing_matrix[:, 1::2] = np.divide(\n",
    "        np_tokens_dirichlet_smoothing_matrix[:, 1::2],\n",
    "        len_corpus_np['abstract'][np.newaxis, :] + alpha\n",
    "    )\n",
    "    \n",
    "    return tokens_dirichlet_smoothing_matrix\n",
    "\n",
    "tokens_dirichlet_smoothing_matrix = get_tokens_dirichlet_smoothing_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_search(query: str, max_result_count: int):\n",
    "    \"\"\"\n",
    "    Finds relevant documents to query\n",
    "\n",
    "    Parameters\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
    "                        notice that if max_result_count = -1, then you have to return all docs\n",
    "\n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    list\n",
    "    Retreived documents with snippet\n",
    "    \"\"\"\n",
    "    weight = 0.5\n",
    "    scores = defaultdict(lambda: 1)\n",
    "    q_tokens = preprocessor.preprocess(query)\n",
    "    for doc_id in corpus.keys():\n",
    "        title_score = weight\n",
    "        abstract_score = (1-weight)\n",
    "        for token in q_tokens:\n",
    "            title_score *= tokens_dirichlet_smoothing_matrix[doc_id, 'title'][token]\n",
    "            abstract_score *= tokens_dirichlet_smoothing_matrix[doc_id, 'abstract'][token]\n",
    "        scores[doc_id] = title_score + abstract_score\n",
    "    top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:max_result_count]\n",
    "    return [doc_id for doc_id, _ in top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>4.\n",
    "ارزیابی عملکرد سامانه\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش معیارهای زیر را برای سیستم‌های طراحی شده پیاده‌سازی کنید. سپس در بخش آخر با فراخوانی توابع پیاده‌سازی شده و ورودی دادن مقادیر موجود در فایل\n",
    "validation.json\n",
    "به عنوان ورودی \n",
    "actual\n",
    "توابع، می‌توانید معیارها را برای سیستم‌های بازیابی خودتان بدست بیاورید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, actual, predicts):\n",
    "        self.actual = actual\n",
    "        self.queries = predicts\n",
    "\n",
    "    def evaluate(self, require_scores):\n",
    "        \"\"\"\n",
    "        Prints require scores for actual and predicts array\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        require_scores : List[str]\n",
    "            Scores required to calculated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        [\"precision\", \"recall\", \"f1\", \"map\", \"ndcg\", \"mrr\"]\n",
    "        for require_score in require_scores:\n",
    "            scores = None\n",
    "            if require_score == 'precision':\n",
    "                scores = self.precision()\n",
    "            elif require_score == 'recall':\n",
    "                scores = self.recall()\n",
    "            elif require_score == 'f1':\n",
    "                scores = self.f1()\n",
    "            elif require_score == 'map':\n",
    "                scores = self.map()\n",
    "            elif require_score == 'ndcg':\n",
    "                scores = self.ndcg()\n",
    "            elif require_score == 'mrr':\n",
    "                scores = self.mrr()\n",
    "            else:\n",
    "                continue\n",
    "            print(f'{require_score}: {np.mean(scores)}')\n",
    "\n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        Calculates the precision of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The precision of the predicted results\n",
    "        \"\"\"\n",
    "        precisions = []\n",
    "        for i in range(len(self.queries)):\n",
    "            query_top_k = self.queries[i]\n",
    "            actual_top_k = self.actual[i]\n",
    "            precisions.append(\n",
    "                len(set(query_top_k) & set(actual_top_k)) \n",
    "                /\n",
    "                len(query_top_k)\n",
    "            )\n",
    "        \n",
    "        return precisions\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Calculates the recall of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The recall of the predicted results\n",
    "        \"\"\"\n",
    "        recalls = []\n",
    "        for i in range(len(self.queries)):\n",
    "            query_top_k = self.queries[i]\n",
    "            actual_top_k = self.actual[i]\n",
    "            recalls.append(\n",
    "                len(set(query_top_k) & set(actual_top_k)) \n",
    "                /\n",
    "                len(actual_top_k)\n",
    "            )\n",
    "\n",
    "        return recalls\n",
    "\n",
    "    def f1(self):\n",
    "        \"\"\"\n",
    "        Calculates the F1 score of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The F1 score of the predicted results\n",
    "        \"\"\"\n",
    "        f1_scores = []\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        for i in range(len(precision)):\n",
    "            f1_scores.append(\n",
    "                2 * precision[i] * recall[i] \n",
    "                /\n",
    "                (precision[i] + recall[i])\n",
    "            )\n",
    "\n",
    "        return f1_scores\n",
    "\n",
    "    def map(self):\n",
    "        \"\"\"\n",
    "        Calculates the Mean Average Precision of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The Mean Average Precision of the predicted results\n",
    "        \"\"\"\n",
    "        map_scores = []\n",
    "        for j in range(len(self.queries)):\n",
    "            query_top_k = self.queries[j]\n",
    "            actual_top_k = self.actual[j]\n",
    "            correct_selections_set = set(query_top_k) & set(actual_top_k)\n",
    "            m_j = len(correct_selections_set)\n",
    "            map_scores.append(0)\n",
    "            tp_conter = 0\n",
    "            for i in range(len(query_top_k)):\n",
    "                if query_top_k[i] in correct_selections_set:\n",
    "                    tp_conter += 1\n",
    "                    map_scores[j] += tp_conter / (i + 1) / m_j\n",
    "            \n",
    "\n",
    "        return map_scores\n",
    "\n",
    "    def ndcg(self):\n",
    "        \"\"\"\n",
    "        Calculates the Normalized Discounted Cumulative Gain (NDCG) of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The NDCG of the predicted results\n",
    "        \"\"\"\n",
    "        ndcg_scores = []\n",
    "        for j in range(len(self.queries)):\n",
    "            query_top_k = self.queries[j]\n",
    "            actual_top_k_set = set(self.actual[j])\n",
    "            normalization_factor = 0\n",
    "            ndcg_scores.append(0)\n",
    "            for m in range(len(query_top_k)):\n",
    "                value = 1 / np.log2(2 + m)\n",
    "                normalization_factor += value\n",
    "                if query_top_k[m] in actual_top_k_set:\n",
    "                    ndcg_scores[j] += value\n",
    "            ndcg_scores[j] /= normalization_factor\n",
    "\n",
    "        return ndcg_scores\n",
    "\n",
    "    def mrr(self):\n",
    "        \"\"\"\n",
    "        Calculates the Mean Reciprocal Rank of the predicted results\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actual : List[List[str]]\n",
    "            The actual results\n",
    "        predicted : List[List[str]]\n",
    "            The predicted results\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The MRR of the predicted results\n",
    "        \"\"\"\n",
    "        mrr_scores = []\n",
    "        for i in range(len(self.queries)):\n",
    "            query_top_k = self.queries[i]\n",
    "            actual_top_k = self.actual[i]\n",
    "            actual_top_1 = actual_top_k[0]\n",
    "            try:\n",
    "                index = query_top_k.index(actual_top_1)\n",
    "                mrr_scores.append(1 / (index + 1))\n",
    "            except ValueError:\n",
    "                mrr_scores.append(0)                \n",
    "\n",
    "        return mrr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_scores = [\"precision\", \"recall\", \"f1\", \"map\", \"ndcg\", \"mrr\"]\n",
    "\n",
    "# Read actuals and queries from validation.json file\n",
    "actuals = []\n",
    "queries = []\n",
    "\n",
    "with open('validation.json', 'r') as file:\n",
    "    val_json = json.loads(file.read())\n",
    "actuals.extend(val_json.values())\n",
    "queries.extend(val_json.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.3666666666666667\n",
      "recall: 0.3666666666666667\n",
      "f1: 0.3666666666666667\n",
      "map: 0.5331569664902999\n",
      "ndcg: 0.3947057678195898\n",
      "mrr: 0.2740740740740741\n"
     ]
    }
   ],
   "source": [
    "predicts = [search(query, 10, method='ltc-lnc-tat') for query in queries]\n",
    "\n",
    "eval_vsm = Evaluation(actuals, predicts)\n",
    "eval_vsm.evaluate(require_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.65\n",
      "recall: 0.65\n",
      "f1: 0.65\n",
      "map: 0.8044690098261528\n",
      "ndcg: 0.6878623965624748\n",
      "mrr: 0.32010582010582006\n"
     ]
    }
   ],
   "source": [
    "predicts = [search(query, 10, method='okapi25') for query in queries]\n",
    "\n",
    "eval_bm = Evaluation(actuals, predicts)\n",
    "eval_bm.evaluate(require_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.75\n",
      "recall: 0.75\n",
      "f1: 0.75\n",
      "map: 0.8724546222810111\n",
      "ndcg: 0.7696766746699987\n",
      "mrr: 0.2876984126984127\n"
     ]
    }
   ],
   "source": [
    "predicts = [lm_search(query, 10) for query in queries]\n",
    "\n",
    "eval_lm = Evaluation(actuals, predicts)\n",
    "eval_lm.evaluate(require_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "information-XQwb_hBV-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
